# Financial Document OCR RAG System

A production-ready Retrieval-Augmented Generation (RAG) pipeline for extracting and querying financial documents, with full observability and evaluation powered by **Weights & Biases Weave**.

## Overview

This project demonstrates how to build a trustworthy AI system for financial services that combines:
- **LlamaIndex** for intelligent OCR and structured data extraction
- **OpenAI** for embeddings and language generation
- **Pinecone** for vector storage and semantic search
- **Weave** for end-to-end tracing, evaluation, and Model Risk Management (MRM) compliance

Perfect for financial institutions that need to extract, query, and validate information from income statements, balance sheets, and other financial documents.

## The Pipeline

The system follows a 4-stage RAG pipeline, with **Weave observability at every step**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG PIPELINE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ğŸ“„ EXTRACTION (LlamaIndex + Weave)
   â””â”€> Financial document â†’ LlamaIndex OCR â†’ Structured data
       â””â”€> @weave.op() tracks extraction process

2. ğŸ”¢ EMBEDDING & STORAGE (OpenAI + Pinecone + Weave)
   â””â”€> Text chunks â†’ OpenAI embeddings â†’ Pinecone vector store
       â””â”€> @weave.op() tracks chunking and embedding

3. ğŸ” RETRIEVAL (Pinecone + Weave)
   â””â”€> Query â†’ Embedding â†’ Vector search â†’ Relevant chunks
       â””â”€> @weave.op() tracks retrieval quality

4. ğŸ¤– GENERATION (GPT-4 + Weave)
   â””â”€> Query + Context â†’ LLM â†’ Answer
       â””â”€> @weave.op() tracks generation and faithfulness
```

### How Weave Powers Observability

**Every operation is traced** using Weave's `@weave.op()` decorator:

```python
@weave.op()
def extract_documents(sources):
    # LlamaIndex extraction logic
    # Weave automatically logs inputs, outputs, latency
    ...

@weave.op()
def create_embeddings(texts):
    # OpenAI embedding logic
    # Weave tracks token usage, model versions
    ...

@weave.op()
def generate_answer(query, context):
    # GPT-4 generation logic
    # Weave captures full prompt, response, tokens
    ...
```

The entire `RagModel` inherits from `weave.Model`, which provides:
- **Automatic versioning** when you change parameters
- **Configuration tracking** (chunk size, embedding model, LLM settings)
- **End-to-end traces** linking extraction â†’ embedding â†’ retrieval â†’ generation

## Weave for MRM & Evaluation

Beyond tracing, Weave enables rigorous evaluation for Model Risk Management:

### 1. Custom Scorers
Define metrics that matter for financial accuracy:

```python
@weave.op()
def recall_at_k(retrieved_docs, relevant_docs, k):
    """How many important documents did we retrieve?"""
    ...

@weave.op()
def numeric_consistency(answer, context):
    """Do numbers in the answer appear in source documents?"""
    ...

@weave.op()
def faithfulness(answer, context):
    """Is the answer grounded in retrieved context?"""
    ...
```

### 2. Dataset Versioning
Create reusable, versioned evaluation datasets:

```python
dataset = weave.Dataset(
    name="financial_rag_eval",
    rows=[
        {"query": "What is net profit?", "expected_docs": [...], ...},
        {"query": "What are total expenses?", ...},
    ]
)
weave.publish(dataset)  # Versioned and shareable
```

### 3. Evaluation Comparison
Run evaluations across model versions and compare:
- **Quantitative metrics**: Recall, MRR, faithfulness, numeric consistency
- **Qualitative outputs**: Side-by-side answer comparison
- **Performance**: Latency and token usage

Weave automatically tracks which model version, which dataset version, and which prompt version produced each result.

## Tech Stack

| Component | Technology |
|-----------|------------|
| **OCR & Extraction** | LlamaIndex (LlamaExtract) |
| **Embeddings** | OpenAI `text-embedding-3-small` |
| **Vector Database** | Pinecone (Serverless) |
| **LLM** | OpenAI GPT-4 |
| **Observability & Eval** | Weights & Biases Weave |
| **UI** | Streamlit |

## Setup

### 1. Prerequisites

- Python 3.13+
- API Keys for:
  - OpenAI
  - Pinecone
  - LlamaIndex Cloud

### 2. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd OCR_MRM

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Environment Configuration

Create a `.env` file in the root directory:

```bash
# OpenAI
OPENAI_API_KEY=your_openai_key_here

# Pinecone
PINECONE_API_KEY=your_pinecone_key_here

# LlamaIndex Cloud
LLAMA_CLOUD_API_KEY=your_llama_cloud_key_here
```

### 4. Configure Weave Project

Update the Weave project name in `streamlit_app.py` (line 227):

```python
weave.init("your-weave-project-name")  # Change this to your project
```

### 5. Run the Demo

```bash
# Using the start script
./start.sh

# Or directly
streamlit run streamlit_app.py
```

The app will open at `http://localhost:8501`

## Project Structure

```
OCR_MRM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llamaindex/         # LlamaIndex extraction logic
â”‚   â”‚   â”œâ”€â”€ extractor.py    # Document OCR and structured extraction
â”‚   â”‚   â””â”€â”€ llama.py        # LlamaIndex configuration
â”‚   â”œâ”€â”€ rag/                # RAG pipeline components
â”‚   â”‚   â”œâ”€â”€ chunker.py      # Text chunking with overlap
â”‚   â”‚   â”œâ”€â”€ embed.py        # OpenAI embedding creation
â”‚   â”‚   â”œâ”€â”€ vectore_store.py # Pinecone vector store wrapper
â”‚   â”‚   â””â”€â”€ retriever.py    # Vector search and retrieval
â”‚   â”œâ”€â”€ weave/              # Weave model and prompts
â”‚   â”‚   â”œâ”€â”€ model.py        # Main RagModel (weave.Model)
â”‚   â”‚   â””â”€â”€ prompt.py       # Prompt templates
â”‚   â””â”€â”€ evaluation/         # Evaluation framework
â”‚       â”œâ”€â”€ dataset_creator.py   # Synthetic data generation
â”‚       â”œâ”€â”€ evaluator.py         # Custom scorers
â”‚       â””â”€â”€ weave_native_eval.py # Weave evaluation runner
â”œâ”€â”€ data/
â”‚   â””â”€â”€ images/             # Sample financial documents
â”œâ”€â”€ streamlit_app.py        # Interactive demo UI
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ start.sh               # Launch script
```

## Usage

### Demo UI (Streamlit)

The Streamlit app provides a 4-step interactive demo:

1. **Upload & Extract**: Upload a financial document, extract structured data with LlamaIndex
2. **Embed & Upsert**: Create embeddings and store in Pinecone
3. **Ask & Answer**: Query the document using natural language
4. **Evaluate**: View evaluation methodology and comparison views

### Programmatic Usage

```python
import weave
from src.weave.model import RagModel

# Initialize Weave
weave.init("your-project-name")

# Create model
model = RagModel(
    index_name="financial-docs",
    namespace="default",
    embedding_model="text-embedding-3-small",
    llm_model="gpt-4o",
    chunk_size=500,
    retriever_top_k=5
)

# Process a document (async)
await model.load_and_process_document("path/to/document.pdf")

# Query the document
result = model.predict("What is the company's net profit?")
print(result["generated_answer"])
```

### Running Evaluations

```python
import weave
from src.evaluation.evaluator import create_evaluation
from src.weave.model import RagModel

weave.init("your-project-name")

# Load evaluation dataset
dataset = weave.ref("your-dataset-ref").get()

# Create model
model = RagModel(index_name="financial-docs", namespace="default")

# Run evaluation
evaluation = create_evaluation(dataset)
results = await evaluation.evaluate(model)

# Results are logged to Weave for comparison
```

## Why Weave?

For financial services, **traceability and validation are critical**:

âœ… **Full Lineage**: Every prediction traces back to exact model version, prompt, and data  
âœ… **Evaluation Framework**: Quantify quality with custom metrics  
âœ… **Version Control**: Automatic versioning of models, datasets, and prompts  
âœ… **MRM Compliance**: Provides audit trail for model risk management  
âœ… **Cost Tracking**: Monitor token usage and API costs  
âœ… **Performance**: Track latency across the pipeline  

## License

MIT License - See LICENSE file for details

## Contributing

Contributions welcome! Please open an issue or submit a pull request.

---

Built with â¤ï¸ using LlamaIndex, OpenAI, Pinecone, and Weights & Biases Weave

